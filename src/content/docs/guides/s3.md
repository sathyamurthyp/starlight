---
title: AWS S3 Cost Optimization Techniques
description: A guide in my new Starlight docs site.
---

### AWS S3 Cost Optimization Techniques

Here's a breakdown of 25 techniques, categorized for clarity, with code snippets, policy examples, and explanations:

#### 1. **Storage Class Optimization**

   *   **Description**: Choosing the right storage class based on access patterns.
   *   **Techniques**:
      *   **S3 Intelligent-Tiering**: Automatically moves data between tiers based on access patterns.        
      *   **S3 Standard**: For frequently accessed data.
      *   **S3 Standard-IA**: For infrequently accessed data.
      *   **S3 One Zone-IA**: For infrequently accessed data, stored in a single AZ.
      *   **S3 Glacier**: For long-term archives with retrieval times from minutes to hours.
      *   **S3 Glacier Deep Archive**: For long-term archives with retrieval times up to 12 hours, lowest cost.
   *   **Example**: Transitioning objects to S3 Glacier after 90 days of inactivity using S3 Lifecycle rules.

```json
{
    "Rules": [
        {
            "ID": "TransitionToGlacier",
            "Filter": {
                "Prefix": ""
            },
            "Status": "Enabled",
            "Transitions": [
                {
                    "Date": "2025-03-15T00:00:00.000Z",
                    "StorageClass": "GLACIER"
                 }
            ],
             "Expiration": {
                 "Days": 365
              }
        }
    ]
}
```

#### 2. **Lifecycle Policies**

   *   **Description**: Automate object transitions and expirations.
   *   **Techniques**:
      *   **Transition to Infrequent Access**: Move objects to IA tiers after a set period.
      *   **Expire Objects**: Automatically delete objects after a specific time.
      *   **Abort Incomplete Multipart Uploads**: Automatically remove failed uploads.

   *   **Example**: Expire temporary log files after 30 days.
```json
{
    "Rules": [
        {
            "ID": "ExpireLogs",
            "Filter": {
                "Prefix": "logs/"
            },
            "Status": "Enabled",
            "Expiration": {
                "Days": 30
            }
        }
    ]
}
```

#### 3. **Data Compression**

   *   **Description**: Reduce storage footprint and transfer costs.
   *   **Techniques**:
      *   **GZIP Compression**: Compress data before uploading to S3.
      *   **Other Compression Algorithms**: Use algorithms like Snappy, Brotli.
   *   **Example**
   ```python
     import gzip
     import boto3
     s3 = boto3.client('s3')
     with open('mydata.txt', 'rb') as f_in:
        with gzip.open('mydata.txt.gz', 'wb') as f_out:
             f_out.writelines(f_in)
     s3.upload_file('mydata.txt.gz', 'mybucket', 'mydata.txt.gz')
   ```

#### 4. **Data Deduplication**

   *   **Description**: Avoid storing duplicate data.
   *   **Techniques**:
      *   **Implement Logic**: Before uploading, check if the object already exists (e.g. using hash check)

#### 5. **Data Tagging**

   *   **Description**: Categorize and track S3 costs by using tags.
   *   **Techniques**:
      *   **Tag Objects**: Use key-value pairs for cost allocation.
      *   **Tag Buckets**: To organize by team or project.
   *   **Example**
```python
  import boto3
  s3 = boto3.resource('s3')
  bucket = s3.Bucket('mybucket')
  bucket.put_object_tagging(
    Tagging = {
        'TagSet': [
            {
                'Key': 'Project',
                'Value': 'ProjectA'
            },
            {
                'Key': 'Team',
                'Value': 'FinOps'
            }
        ]
      }
  )
```

#### 6. **S3 Inventory**

   *   **Description**: Audit storage usage and identify cost optimization opportunities.
   *   **Techniques**:
      *   **Enable S3 Inventory**: Generate CSV or Parquet reports.
      *   **Analyze Reports**: Identify large objects, uncompressed data, etc.

#### 7. **S3 Analytics**

   *   **Description**: Understand access patterns to inform lifecycle policies.
   *   **Techniques**:
      *   **Enable S3 Storage Class Analytics**: Track access patterns over time.
      *   **Use Insights**: Configure appropriate storage tiering.

#### 8. **S3 Transfer Acceleration**

   *   **Description**: Speed up data transfers over long distances.
   *   **Techniques**:
      *   **Enable Transfer Acceleration**: For large data transfers.
      *   **Cost Consideration**: Evaluate if benefits outweigh the cost.

#### 9. **Request Optimization**

   *   **Description**: Reduce the number of S3 requests.
   *   **Techniques**:
      *   **Batch Operations**: Use S3 Batch Operations for bulk actions.
      *   **Optimize Access Patterns**: Reduce frequent access of small objects.
   *   **Example**
  ```python
    import boto3
    s3 = boto3.client('s3')
    s3.create_job(
        AccountId = '1234567890',
        Operation = {
        'S3PutObjectCopy': {
            'StorageClass': 'STANDARD_IA',
            },
        },
        Report = {
        'Bucket': 'reportbucket',
        'Format': 'Report_CSV_20180820',
            'Enabled': True,
        'Prefix': 'reportprefix'
            },
        Manifest = {
        'Location': {
                'ObjectArn': 'arn:aws:s3:::manifestbucket/manifestfile',
                'ETag': 'manifestEtag'
        },
        'Spec':{
            'Format': 'S3BatchManifest_CSV_20180820',
            'Fields':['Bucket','Key']
            }
        },
            Type ='S3BatchOperations_Copy',
        Description = 'This is sample job'
     )
    ```

#### 10. **Data Replication**

   *   **Description**: Avoid unnecessary data replication.
   *   **Techniques**:
      *   **Use Cross-Region Replication (CRR) wisely**: Only replicate if required.
      *   **Use Same-Region Replication (SRR)** : To replicate within the same region for use cases like DR or Compliance.

#### 11. **S3 Object Lock**
    *   **Description**: Avoid accidental deletion or over-writing of objects with object lock.
    *   **Techniques**:
        *   **Enable Object Lock**:  For compliance and regulatory requirements.
        *   **Use Retention Periods**: Set retention periods with Legal Hold or Governance Mode.

#### 12. **Avoid Over-Provisioning**
    *   **Description**: Use the right size for your requirements, and avoid over-provisioning to prevent unnecessary costs.
     *   **Techniques**:
          *  **Right sizing**: Use only required capacity, and make changes to the size as needed.

#### 13. **Regular Cost Monitoring**
    *   **Description**: Consistently monitor your usage and spending.
    *   **Techniques**:
         *   **AWS Cost Explorer**: To analyze costs trends.
         *   **AWS Budgets**:  Set up alerts for exceeding spending thresholds.

#### 14. **Use VPC Endpoints**
     *   **Description**: Secure your S3 traffic within VPC by using VPC Endpoints
     *   **Techniques**:
        *   **Create VPC Endpoints for S3**: Secure traffic and avoid data transfer costs through internet.

#### 15. **Optimize Data Transfer**
    *   **Description**: Reduce the amount of data being transferred.
    *   **Techniques**:
        *   **Limit Data Downloads**:  Retrieve only the needed parts of the object.
        *   **Batch Download**: Use batch downloads for larger data loads.

#### 16. **Server-Side Encryption**
     *  **Description**: Protect your data at rest by using S3 provided Server Side encryption
     *   **Techniques**:
         *   **Enable SSE-S3/SSE-KMS**: Set up encryption for buckets.
         *   **Use Bucket Policies**:  To enforce encryption on your data.
   *   **Example**
```json
{
    "Version": "2012-10-17",
    "Id": "PutObjectPolicy",
    "Statement": [
        {
            "Sid": "DenyUnEncryptedUploads",
            "Effect": "Deny",
            "Principal": "*",
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::mybucket/*",
            "Condition": {
                "StringNotEquals": {
                    "s3:x-amz-server-side-encryption": "AES256"
                   }
             }
        }
    ]
}
```

#### 17. **Client-Side Encryption**
     * **Description**: Encrypt the data at the client side before uploading to S3 for enhanced security.
     * **Techniques**:
          *  **Encrypt data using Client Side Encryption libraries** : Before data upload to S3.

#### 18. **Audit Access**
    * **Description**: Audit S3 buckets access, to remove unintended permissions.
    * **Techniques**:
         *  **Use AWS IAM Access Analyzer** : To remove unused permissions.

#### 19. **Use S3 Access Points**
     * **Description**: Secure and manage access to your shared datasets in S3 buckets.
     * **Techniques**:
          * **Use S3 Access Points**: Create multiple access points for different applications or users.

#### 20. **Monitor Storage Usage**
      * **Description**: Keep track of your storage usage, and optimize regularly.
      * **Techniques**:
         *  **Use CloudWatch metrics for S3**: Track storage usage, and implement strategies based on the trends.

#### 21. **Avoid Unnecessary API Calls**
       * **Description**: Prevent too many calls to the S3 APIs to reduce costs.
       * **Techniques**:
             * **Use SDK to upload multiple objects**: Instead of uploading one object at a time.

#### 22. **S3 Batch Operations**
    *   **Description**: Perform large scale actions on S3 objects such as copying, tagging etc.
    *   **Techniques**:
        *  **Use S3 Batch Operations** : To run actions on multiple objects, by using a manifest file.

#### 23. **Object Tagging for Automation**
     *   **Description**: Set tags for automation in S3.
     *   **Techniques**:
        *   **Set up Lambda Functions**: To automate workflows based on the tags.

#### 24. **Enable S3 Event Notifications**
      *   **Description**: Get notifications when certain events happen in S3 buckets.
      *   **Techniques**:
            * **Use SNS/SQS**: To send out notifications on specific events.

#### 25. **Regular Cost Reviews**
       *   **Description**: Review S3 costs regularly to make sure they are within budget.
       *   **Techniques**:
            * **Schedule regular cost reviews** : To implement ongoing optimizations.